---
title: "The `mice` imputation flow"
format: 
  html: 
    toc: true
    theme: materia
    number-sections: true
editor: source
---
# tl;dr
## Generating imputations
We need the user be able to specify the 

- data
- number of imputations
- maximum number of iterations
- seed
- imputation method
- the predictor matrix; if relevant with blocks 
- the visit sequence 

Later on, we may extend this to formulae for the imputation models.

## Presentation of imputed data 
The most conscientious way to export the imputed data is to export both the `mids` object from `mice` and present the user with an extracted long format data set. The `mids` object contains all crucial information about the imputation process and the algorithm. The long format data set mimics the original data, but appends a column indicator for the $m$ imputed data sets. 

- Allow for importing a `mids` object
- Generate a long format data set with said `mids` object

A long format does not lull the user into a false sense of security by assuming that a single imputed data set can be used for analysis.

## Analyzing imputed data
With regression as an example, we need to combine the following JASP output to obtain the pooled estimates
![](lm_indic.png)

More combinations are needed to present the user with the same familiar output as above. This is highlighted in the @sec-crucial section. In short, for any output other than the quantity of interest one could choose to:

- omit the output and only present the pooled estimates and combined inference
- aggregate the other output e.g. as follows:
  - normal sampling distribution --> take average and show variance/range
  - non-normal sampling distribution --> show some proxy of the distribution

Never present users with the separate output of one or more imputed data sets - they may be tempted to use them for analysis. If necessary, these results can be obtained by the user by splitting on the imputation indicator.

# Introduction to `mice`
Package `mice` in [R]{.proglang} is a powerful tool for imputing missing data. It is a flexible and easy-to-use package that can handle different types of missing data, including monotone missing data, non-monotone missing data, and missing data in time series. The package is also capable of handling missing data in both continuous and categorical variables.

The `mice` package is based on the concept of multiple imputation, which is a statistical technique that involves creating multiple imputed datasets, analyzing each dataset separately, and then combining the results to obtain a single set of estimates. This approach has been shown to be more accurate and reliable than other imputation methods, such as mean imputation or listwise deletion and can be seen as a quick approximation of a full Bayesian solution. 

The `mice` package is widely used in a variety of fields, including social science, epidemiology, and public health, and is an essential tool for researchers who need to analyze datasets with missing data. `mice` has become the de facto standard for multiple imputation in [R]{.proglang} and is widely used in both academia and industry with over 1 million downloads every year from CRAN. 

---

# Aim of this flow
In this flow, we will provide an overview of the `mice` package and demonstrate how it approaches the stages of imputation, analysis, evaluation and combination. 

---

# Imputation
The `mice` algorithm is an algorithm to generate multiple imputations by chained equations. 

::: {.callout-note}
## Why multiple imputations?
With imputation, some estimation procedure is used to impute (fill in) each missing datum, resulting in a completed dataset that can be analyzed as if the data were completely observed.
We can do this once (single imputation) or multiple times (multiple imputation). 

With multiple imputation, each missing datum is imputed $m \geq 2$ times, resulting in $m$ completed datasets. The $m$ completed datasets are then analyzed separately, and the results are combined to yield valid statistical inference.

Multiple imputation (Rubin, 1987) has some benefits over single imputation:

- it accounts for missing data uncertainty
- it accounts for parameter uncertainty
- can yield valid inference without additional adjustments 
:::

The `mice` algorithm was initially designed as a fully conditional specification (FCS) algorithm, which is a type of imputation algorithm that imputes each variable separately, one-by-one, conditional on all other variables in the dataset. The FCS algorithm is based on the idea that imputing each variable separately allows for more flexibility than imputing all variables simultaneously. The rationale behind this thought is that in `mice`, each incomplete feature in the data can be addressed by a separate imputation model, which can be tailored to the specific characteristics of the feature. An efficient sufficient set of predictors can be chosen for each model, to further optimize the imputation process. 

Sometimes, however, it is not desirable to adopt the chained equations approach to imputation. For example, if multiple features in the data obey a specific deterministic relation, then parts of the data should be jointly imputed, instead of separately. Otherwise, the interdepency of features is ignored by the imputation model, which renders the imputation procedure uncongenial with respect to the analysis. To accommodate this, the `mice` has since deviated from a strictly variable-by-variable architecture and allows for the flexible joint imputation of (blocks of) features. This hybrid approach - where the imputer can assign blocks of features to be jointly imputed, next to other features still being imputed on a variable-by-variable basis - greatly enhances the practical applicability of the `mice` algorithm. If desired, `mice` can serve as fully joint imputation method, by specifying the imputation model to be a multivariate model and all features to be imputed as a single joint block.

---

## Imputation procedure
Let $Y$ be an incomplete column in the data, with

  - $Y_\mathrm{mis}$ denoting the unobserved part;
  - $Y_\mathrm{obs}$ denotes the observed part.

Let $X$ be a set of completely observed covariates. 

---

### Joint model imputation
With joint modeling, imputations are drawn from an assumed joint multivariate distribution. 

- Often a multivariate normal model is used for both continuous and categorical data, 
- Other joint models have been proposed (see e.g. Olkin and Tate, 1961; Van Buuren and van Rijckevorsel, 1992; Schafer, 1997; Van Ginkel et al., 2007; Goldstein et al., 2009; Chen et al., 2011). 

Joint modeling imputations generated under the normal model are usually robust to misspecification of the imputation model (Schafer, 1997; Demirtas et al., 2008), **although transformation towards normality is generally beneficial.**

The procedure for joint modeling is as follows:

1. Specify the joint model $P(Y,X)$
2. Derive $P(Y_\mathrm{mis}|Y_\mathrm{obs},X)$
3. Draw imputations $\dot Y_\mathrm{mis}$ with a Gibbs sampler

---

### Fully conditional specification
Multiple imputation by means of FCS does not start from an explicit multivariate model. With FCS, multivariate missing data is imputed by univariately specifying an imputation model for each incomplete variable, conditional on a set of other (possibly incomplete) variables. 

- the multivariate distribution for the data is thereby implicitly specified through the univariate conditional densities. 
- imputations are obtained by iterating over the conditionally specified imputation models.

The general procedure for fully conditional specification would be to

1. Specify $P(Y^\mathrm{mis} | Y^\mathrm{obs}, X)$
2. Draw imputations $\dot Y^\mathrm{mis}$ with Gibbs sampler

The idea of using conditionally specified models to deal with missing data has been discussed and applied by many authors (see e.g. Kennickell, 1991; Raghunathan and Siscovick, 1996; Oudshoorn et al., 1999; Brand, 1999; Van Buuren et al., 1999; Van Buuren and Oudshoorn, 2000; Raghunathan et al., 2001; Faris et al., 2002; Van Buuren et al., 2006.) 

Comparisons between JM and FCS have been made that indicate that FCS is a useful and flexible alternative to JM when the joint distribution of the data is not easily specified (Van Buuren, 2007) and that similar results may be expected from both imputation approaches (Lee and Carlin, 2010).

In `mice`, the fully conditional specification has been implemented as follows:

1. Specify the imputation models $P(Y_j^\mathrm{mis} | Y_j^\mathrm{obs}, Y_{-j}, X)$, where $Y_{âˆ’j}$ is the set of incomplete variables except $Y_j$
2. Fill in starting values for the missing data. It does not matter what these values are, as long as they are not missing. Random univariate draws have proven to be computationally convenient without hindering the speed of algorithmic convergence.
3. Iterate

---

### Hybrids of JM and FCS
In `mice` we can combine the flexibility of FCS with the appealing theoretical properties of joint modeling. In order to do so, we need to partition the variables into **blocks**

- For example, we might partition $b$ blocks $h = 1,\dots,b$ as follows

  - a single block with $b=1$ would hold a **joint model**:
$$\{Y_1, Y_2, Y_3, Y_4\}, X$$
  - a quadruple block with $b=4$ would be the conventional fully conditional specification algorithm
  $$\{Y_1\},\{Y_2\},\{Y_3\},\{Y_4\}, X$$

  - anything in between would be a hybrid between the joint model and fully conditional specification. For example,
  $$\{Y_1, Y_2, Y_3\},\{Y_4\}, X$$  

---

#### Why is this useful
There are many scenarios where a hybrid imputation procedure would be useful. wewill highlight a few:

- **Imputing squares/nonlinear effects**: In the model $y=\alpha + \beta_1X+\beta_2X^2 + \epsilon$, $X$ and $X^2$ should be imputed jointly (Von Hippel, 2009, Seaman, Bartlett & White, 2012, Vink & Van Buuren, 2013, Bartlett et al., 2015)
- **Compositional data**: The simplex space in compositional data should be considered as. joint, because the relations between the variables are captured in the ratios. Although some clever subsetting of the problem would allow this to be solved with FCS (Vink, 2015, Ch5), it is better to consider the joint distribution in the following nested composition:

$$
\begin{array}{lllllllllllll}
x_0 &=	&x_1		&+	&x_2		&+		&x_3		&+& x_4	& 		& 	& 	&\\
       &  	&= 		&   	&      		& 		&  		&& =		& 		& 	&	&\\
       &  	&x_9    	&   	&      		&		& 		&& x_5	& 		& 	&	&\\
       &  	&+		&   	&      		&		& 		&& +		& 		& 	&	&\\
       &  	&x_{10} 	&   	&      		&		& 	 	&&x_6		&= 		&x_7 	&+&x_8
\end{array}
$$

- **Multivariate PMM**: Imputing a combination of outcomes optimally based on a linear combination of covariates (Cai, Vink & Van Buuren, 2023). 

---

#### Nesting joint modeling in FCS
The following table details $b=2$ blocks. 

b  | h | target            | predictors| type
---|---|-------------------|-----------|------
2  | 1 | $\{Y_1, Y_2, Y_3\}$ | $Y_4, X$ | multivariate
2  | 2 | $Y_4$ | $Y_1, Y_2, Y_3, X$     | univariate

The first block considers the multivariate imputation of the set $(Y_1, Y_2, Y_3)$. The second block considers the univariate imputation of the remaining column $Y_4$. 

---

#### Nesting FCS in FCS
With FCS, the scheme on the previous table would take the following embedded structure:

b  | h | j | target| predictors      | type
---|---|---|-------|------------------|--------
2  | 1 | 1 | $Y_1$ | $Y_2, Y_3, Y_4, X$ | univariate
2  | 1 | 2 | $Y_2$ | $Y_1, Y_3, Y_4, X$ | univariate
2  | 1 | 3 | $Y_3$ | $Y_1, Y_2, Y_4, X$ | univariate
2  | 2 | 1 | $Y_4$ | $Y_1, Y_2, Y_3, X$ | univariate

The first block is a FCS loop within an FCS imputation procedure.

---

#### Benefits of blocks in `mice()`

1. Looping over $b$ blocks instead of looping over $p$ columns. 
2. Only specify $b \times p$ predictor relations and not $p^2$. 
3. Only specify $b$ univariate imputation methods instead  of $p$ methods. 
4. Ability for imputing more than one column at once
5. Simplified overall model specification
  - e.g. sets of items in scales, matching items in longitudinal data, joining data sets, etc.

---

## Selecting predictors 

Under the conventional FCS predictor specification, we could hypothesize the following `predictorMatrix`. 

```{r echo=FALSE}
nam <- c("age", "item1", "item2", "sum_items", "time1", "time2", "time3", "mean_time")
a <- matrix(c(0, 0, 0, 1, 0, 0, 0, 1,
              1, 0, 1, 0, 0, 0, 0, 1,
              1, 1, 0, 0, 0, 0, 0, 1,
              0, 1, 1, 0, 0, 0, 0, 0,
              1, 0, 0, 1, 0, 1, 1, 0,
              1, 0, 0, 1, 1, 0, 1, 0,
              1, 0, 0, 1, 1, 1, 0, 0,
              0, 0, 0, 0, 1, 1, 1, 0),
              byrow = TRUE, nrow = 8, 
            dimnames = list(nam, nam))
print(a)
```

In this predictor matrix, the columns indicate the predictor features and the rows are the target features. The value of 1 indicates that the column is a predictor in the imptutaion model for the row.

Under the outlined hybrid blocked imputation approach, however, we could simplify these specifications into the following blocks and predictor relations. 

```{r}
blocks <- list(age = "age", 
               A = c("item1", "item2", "sum_items"), 
               B = c("time1", "time2", "time3", "mean_time"))
```
```{r echo = FALSE}
namr <- c("age", "Items", "Time")
b <- matrix(c(0, 0, 0, 1, 0, 0, 0, 1,
              1, 0, 0, 0, 0, 0, 0, 1,
              1, 0, 0, 1, 0, 0, 0, 0),
              byrow = TRUE, nrow = 3, 
            dimnames = list(namr, nam))
print(b)
```

Fully conditionally specified predictor matrices can easily be generated as follows:

```{r}
mice::make.predictorMatrix(mice::boys)
```

---

## Selecting an imputation method
The `mice` package provides a variety of imputation methods for different types of data. The imputation method can be specified for each imputation model. Default for continuous data is predictive mean matching (PMM), for binary data logistic regression, and for unordered categorical data polytomous regression.

The `mice` algorithm needs to know which imputation method to use for each variable. The imputation method is specified in the `method` argument of the `mice()` function. The `method` argument is a list that contains the imputation method for each variable in the dataset. The imputation method can be specified as a character string or as a function.

A default `method` vector can be created as follows:

```{r}
```{r}
mice::make.method(mice::boys)
```
`age` is left intentionally empty because the column is completely observed and needs no imputation. 

---

## Determining the visit sequence
By default, `mice` iterates over the columns in a dataset in the order they appear. However, the order in which the columns are imputed can have a significant impact on the quality of the imputations. For example, if a total is updated before the imputation of its components, the imputation of any source the total is dependent on, will be inefficient, or even incorrect.

A default `visitSequence` can be created as follows:
```{r}
mice::make.visitSequence(mice::boys)
```

---

# Analysis of imputed data
After the imputation process is complete, the imputed datasets can be analyzed using standard statistical methods. Van Buuren (2020) provides the following schematic overview of the multiple imputation data analysis pipeline.

![](https://stefvanbuuren.name/fimd/fig/ch01-miflow-1.png){width=80%}

In step 1, we create several $m$ complete versions of the data 
by replacing the missing values by 
plausible data\index{plausible imputations} values. The task 
of step 2 is to estimate the parameters of scientific or commercial 
interest from each imputed dataset. Step 3 involves pooling the 
$m$ parameter estimates into one estimate, and obtaining an estimate 
of its variance. The results allow us to arrive at valid decisions 
from the data, accounting for the missing data and having the correct
type I error rate.

These steps relate the following table:

| Class | Name | Produced by | Description                            |
|-------|------|-------------|----------------------------------------|
| `mids`  | `imp`  | `mice::mice()`      | multiply imputed dataset               |
| `mild`  | `idl`  | `mice::complete()`  | multiply imputed list of data          |
| `mira`  | `fit`  | `mice::with()`      | multiple imputation repeated analyses  |
| `mipo`  | `est`  | `mice::pool()`      | multiple imputation pooled results     |

A standard workflow to generate 5 imputations with `mice` and giving the algorithm 10 iterations would be:
```{r}
library(magrittr)
library(purrr)
library(mice)
```
```{r generateimps, cache=TRUE}
meth <- make.method(boys)
pred <- make.predictorMatrix(boys)
vis  <- make.visitSequence(boys)
imp <- mice(boys, m = 5, maxit = 10, 
            method = meth, 
            predictorMatrix = pred, 
            visitSequence = vis, 
            seed = 123) # for reproducibility
```
Then, if we fit the model `age ~ height + weight` on each imputed dataset, we would use the following code:
```{r workflow_old}
fit <- with(imp, lm(age ~ hgt + wgt)) # obtain 
fit
class(fit)
```
or the following pipe
```{r workflow_mild}
fit_2 <- imp |>
  complete("all") |> # obtain object of class mild
  map(~.x %$% lm(age ~ hgt + wgt)) # obtain object of class mira for each idl
fit_2
class(fit_2)
```
Pooling the model parameters happens by default conform the combination rules proposed by Rubin (1987, p76). Rubin defined $Q$ as the quantity of interest (possibly a vector) and $U$ as its variance. With multiple imputation, $m$ complete data estimates can be averaged as
$$\bar{Q}=\frac{1}{m}\sum^{m}_{l=1}{ \hat{Q}}_{l},$$

where $\hat Q_l$ is an estimate of $Q$ from the $l$-th imputed
data set. Let $\bar U_l$ be the estimated variance-covariance matrix of
$\hat Q_l$. The complete data variances of $Q$ can be combined by

$$\bar{U}=\frac{1}{m}\sum^{m}_{l=1}{ {\bar U}}_{l}.$$
The variance between the complete data estimates can be calculated as

$$B=\frac{1}{m-1}\sum^{m}_{l=1}(\hat{ Q}_l-\bar{Q})^\prime(\hat{ Q}_l-\bar{Q}).$$

The total variance of $({ Q}-\bar{Q})$ is then defined as 

$$T=\bar{U}+B+B/m.$$

Obtaining the pooled estimates from either workflow is then straightforward with the `mice::pool()` function
```{r}
pool(fit)
pool(fit_2)
class(fit)
```
and a `mice::summary.mipo()` is available to obtain statistical tests and confidence intervals.
```{r}
fit |>
  pool() |>
  summary(conf.int = TRUE)

fit_2 |>
  pool() |>
  summary(conf.int = TRUE)
```

# Evaluation of algorithmic convergence
The `mice` algorithm is an iterative algorithm that generates imputations by iteratively cycling over the incomplete variables in the dataset, where every iteration would represent a new cycle according to the `visitSequence`. The algorithm stops when the maximum number of iterations `maxit` is reached. At this point, the algorithm may not have converged and closer inspection is needed. `mice` provides a `mice::plot()` function to generate trace plots based on the means and variances of the multiple chains in an object of class `mids`.  

```{r}
plot(imp)
```

As you can see, convergence is not optimal here because the deterministic relation between `wgt`, `hgt` and `bmi` is not jointly considered. But that is not the focus of this document, so we ignore the nonconvergence for now. 

# Evaluation of the imputations
The imputations can be evaluated using a variety of methods, including graphical methods, statistical tests, and comparisons with other imputation methods. The `mice` package provides a variety of functions for evaluating the imputed and the incomplete data, including `mice::densityplot()`, `mice::xyplot()`, `mice::stripplot()`, `mice::bwplot()`, `mice::fluxplot()`, `mice::md.pattern()`.

```{r}
densityplot(imp)
xyplot(imp, age ~ tv)
stripplot(imp)
bwplot(imp)
fluxplot(boys)
md.pattern(boys)
```
A similar but more flexible 'Grammar of Graphics' approach can be obtained by [the `ggmice` package.](https://cran.r-project.org/web/packages/ggmice/vignettes/ggmice.html). But I'll leave that out for now. 

# What JASP output is crucial? {#sec-crucial}
Based on the above outline of the procedure and the standard (i.e. minimum) set of evaluations and plots to be available, the following JASP output is needed to perform the pooling steps:

## Linear regression
Take the following output from JASP as an example.
![](lm.png)
With `R` we would do the following:
```{r}
data <- read.csv("Album Sales.jasp.csv")
data %$% 
  lm(sales ~ airplay) %>% 
  summary
```

## Pooling regression output
### Coefficients and their tests
To illustrate the `mice` workflow, we induce some missingness in the data with `mice::ampute()`. In `mice` we would do the following:
```{r}
set.seed(123)
imp <- ampute(data)$amp |> # make some missings because there are none
  mice(printFlag=FALSE)    # impute, but omit iteration history printing
idl <- imp |>
  complete("all") # obtain object of class mild

idl |>
  map(~.x %$% lm(sales ~ airplay)) |> # run the model on each imputed dataset
  pool() %>%  # pool the results
  summary(conf.int = TRUE) # obtain the combined inference
```
This yields part of the standard SPSS output. The pooled estimates are based on the combination of the following model output by means of Rubin's rules.

### Seperate output per imputed data set
```{r}
idl |>
  map(~.x %$% lm(sales ~ airplay) %>% summary(conf.int = TRUE))
```

### R-squared
We can grab any necessary values from the `mice` imputations as follows. For example, to obtain the $R^2$ values for each imputed dataset, we would do the following:
```{r}
idl |>
  map(~.x %$% lm(sales ~ airplay) %>% summary) |>
  map_dbl(~.$r.squared)  # Extract R^2 for each model
```

Pooling $R^2$ values isn't directly supported by most statistical methodologies because 
is a measure of model fit rather than a parameter estimate. However, we could consider reporting the range, mean, or median of these values as a descriptive statistic of how the models perform across the imputed datasets. I believe Joost van Ginkel (LeidenUniv) has some methodological evaluations about combination approaches for $R^2$ values.

### Overall model ANOVA
Obtaining an estimate of the pooled ANOVA could be done as follows, by comparing the full model to the nested *empty* model. 
```{r}
# Needs to use the with() workflow
fit1 <- with(imp, lm(sales ~ 1)) # intercept only
fit2 <- with(imp, lm(sales ~ airplay)) # intercept and airplay
D1(fit2, fit1) # multivariate Wald test
D2(fit2, fit1) # combining test statistic (least assumptions AND power)
D3(fit2, fit1) # likelihood-ratio test
```

There are also other ways of combining the ANOVA, such as multiplying the $m$ F-statistics with their degrees of freedom, and then pooling according an approximation of the $\chi^2$ distribution (see e.g. [the miceadds::micombine.F function](https://cran.r-project.org/web/packages/miceadds/miceadds.pdf)). 
```{r}
library(miceadds)
Fs <- idl |>
  map(~.x %$% anova(lm(sales ~ airplay))) |>
  map_dbl(~.$F[1])
dfs <- idl |>
  map(~.x %$% anova(lm(sales ~ airplay))) |>
  map_dbl(~.$Df[1])
miceadds::micombine.F(Fs, median(dfs))
```
which is equivalent to the D2-statistic in the above model-comparison outline. 

Also relevant: [https://doi.org/10.1027/1614-2241/a000111](https://doi.org/10.1027/1614-2241/a000111)

